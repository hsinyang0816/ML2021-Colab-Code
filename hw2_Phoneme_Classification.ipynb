{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hw2_Phoneme_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsinyang0816/ML2021-Colab-Code/blob/master/hw2_Phoneme_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYlaRwNu7ojq"
      },
      "source": [
        "# **Homework 2-1 Phoneme Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emUd7uS7crTz"
      },
      "source": [
        "## The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n",
        "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n",
        "\n",
        "This homework is a multiclass classification task, \n",
        "we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n",
        "\n",
        "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVUGfWTo7_Oj"
      },
      "source": [
        "## Download Data\n",
        "Download data from google drive, then unzip it.\n",
        "\n",
        "You should have `timit_11/train_11.npy`, `timit_11/train_label_11.npy`, and `timit_11/test_11.npy` after running this block.<br><br>\n",
        "`timit_11/`\n",
        "- `train_11.npy`: training data<br>\n",
        "- `train_label_11.npy`: training label<br>\n",
        "- `test_11.npy`:  testing data<br><br>\n",
        "\n",
        "**notes: if the google drive link is dead, you can download the data directly from Kaggle and upload it to the workspace**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzkiMEcC3Foq",
        "outputId": "4f6914a0-c01b-4f32-ffc4-5f36ba06b064"
      },
      "source": [
        "!gdown --id '1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR' --output data.zip\n",
        "!unzip data.zip\n",
        "# %cd drive/MyDrive/ML2021/HW2/timit_11_v2/\n",
        "# !ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/ML2021/HW2/timit_11_v2\n",
            "model.ckpt  prediction.csv  timit_11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "walcuisXsGVs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "769cdefa-3d85-4341-d5b2-0effe95f9ac9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L_4anls8Drv"
      },
      "source": [
        "## Preparing Data\n",
        "Load the training and testing data from the `.npy` file (NumPy array)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJjLT8em-y9G",
        "outputId": "25f62001-300c-4726-a01b-ca0e50246884"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "print('Loading data ...')\n",
        "\n",
        "data_root='./timit_11/'\n",
        "train = np.load(data_root + 'train_11.npy')\n",
        "train_label = np.load(data_root + 'train_label_11.npy')\n",
        "test = np.load(data_root + 'test_11.npy')\n",
        "\n",
        "print('Size of training data: {}'.format(train.shape))\n",
        "print('Size of testing data: {}'.format(test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data ...\n",
            "Size of training data: (1229932, 429)\n",
            "Size of testing data: (451552, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us5XW_x6udZQ"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjf5EcmJtf4e"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TIMITDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.data = torch.from_numpy(X).float()\n",
        "        if y is not None:\n",
        "            y = y.astype(np.int)\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "        # Normalize features\n",
        "        self.data[:, :] = \\\n",
        "            (self.data[:, :] - self.data[:, :].mean(dim=0, keepdim=True)) \\\n",
        "            / self.data[:, :].std(dim=0, keepdim=True)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is not None:\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otIC6WhGeh9v"
      },
      "source": [
        "Split the labeled data into a training set and a validation set, you can modify the variable `VAL_RATIO` to change the ratio of validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYqi_lAuvC59",
        "outputId": "481113e2-9db0-4dd0-a955-2b713d28eba5"
      },
      "source": [
        "VAL_RATIO = 0.1\n",
        "\n",
        "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
        "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
        "print('Size of training set: {}'.format(train_x.shape))\n",
        "print('Size of validation set: {}'.format(val_x.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set: (1106938, 429)\n",
            "Size of validation set: (122994, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbCfclUIgMTX"
      },
      "source": [
        "Create a data loader from the dataset, feel free to tweak the variable `BATCH_SIZE` here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUCbQvqJurYc"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_set = TIMITDataset(train_x, train_y)\n",
        "val_set = TIMITDataset(val_x, val_y)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SY7X0lUgb50"
      },
      "source": [
        "Cleanup the unneeded variables to save memory.<br>\n",
        "\n",
        "**notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later<br>the data size is quite huge, so be aware of memory usage in colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8rzkGraeYeN",
        "outputId": "431ef519-f4ab-4541-d3bf-25bf6fd9f0dd"
      },
      "source": [
        "import gc\n",
        "\n",
        "del train, train_label, train_x, train_y, val_x, val_y\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "153"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRqKNvNZwe3V"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYr1ng5fh9pA"
      },
      "source": [
        "Define model architecture, you are encouraged to change and experiment with the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbZrwT6Ny0XL"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.layer1 = nn.Linear(429, 1024)\n",
        "        self.layer2 = nn.Linear(1024, 2048)\n",
        "        self.layer3 = nn.Linear(2048, 4096)\n",
        "        self.layer4 = nn.Linear(4096, 2048)\n",
        "        self.layer5 = nn.Linear(2048, 1024)\n",
        "        self.layer6 = nn.Linear(1024, 512)\n",
        "        self.layer7 = nn.Linear(512, 256)\n",
        "        #self.layer8 = nn.Linear(128, 128)\n",
        "        self.out = nn.Linear(256, 39)\n",
        "        self.dropout1 = nn.Dropout(0.36)\n",
        "        self.dropout2 = nn.Dropout(0.57)\n",
        "        self.dropout3 = nn.Dropout(0.67)\n",
        "        self.dropout4 = nn.Dropout(0.62)\n",
        "        self.dropout5 = nn.Dropout(0.54)\n",
        "        self.dropout6 = nn.Dropout(0.43)\n",
        "        self.dropout7 = nn.Dropout(0.36)\n",
        "        #self.dropout8 = nn.Dropout(0.3)\n",
        "        self.bn1 = nn.BatchNorm1d(1024) \n",
        "        self.bn2 = nn.BatchNorm1d(2048) \n",
        "        self.bn3 = nn.BatchNorm1d(4096)\n",
        "        self.bn4 = nn.BatchNorm1d(2048)\n",
        "        self.bn5 = nn.BatchNorm1d(1024)\n",
        "        self.bn6 = nn.BatchNorm1d(512)\n",
        "        self.bn7 = nn.BatchNorm1d(256)\n",
        "        #self.bn8 = nn.BatchNorm1d(128) \n",
        "\n",
        "        #self.act_fn = nn.Sigmoid()\n",
        "        self.act_fn = nn.ReLU()\n",
        "        #self.act_fn = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.act_fn(x)\n",
        "        #x = self.dropout(x)  \n",
        "\n",
        "        x = self.layer2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.act_fn(x)\n",
        "        #x = self.dropout(x)\n",
        "        \n",
        "        x = self.layer3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.act_fn(x)\n",
        "        #x = self.dropout(x)\n",
        "        \n",
        "        x = self.layer4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.dropout4(x)\n",
        "        x = self.act_fn(x)\n",
        "        #x = self.dropout(x)\n",
        "\n",
        "        x = self.layer5(x)\n",
        "        x = self.bn5(x)\n",
        "        x = self.dropout5(x)\n",
        "        x = self.act_fn(x)\n",
        "        #x = self.dropout(x)\n",
        "\n",
        "        x = self.layer6(x)\n",
        "        x = self.bn6(x)\n",
        "        x = self.dropout6(x)\n",
        "        x = self.act_fn(x)\n",
        "        #x = self.dropout(x)\n",
        "\n",
        "        x = self.layer7(x)\n",
        "        x = self.bn7(x)\n",
        "        x = self.dropout7(x)\n",
        "        x = self.act_fn(x)\n",
        "        #x = self.dropout(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRYciXZvPbYh"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y114Vmm3Ja6o"
      },
      "source": [
        "#check device\n",
        "def get_device():\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEX-yjHjhGuH"
      },
      "source": [
        "Fix random seeds for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88xPiUnm0tAd"
      },
      "source": [
        "# fix random seed\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  \n",
        "    np.random.seed(seed)  \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbBcBXkSp6RA"
      },
      "source": [
        "Feel free to change the training parameters here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTp3ZXg1yO9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c017f091-b74d-4b13-d092-586f86c13546"
      },
      "source": [
        "# fix random seed for reproducibility\n",
        "same_seeds(0)\n",
        "\n",
        "# get device \n",
        "device = get_device()\n",
        "print(f'DEVICE: {device}')\n",
        "\n",
        "# training parameters\n",
        "num_epoch = 1000              # number of training epoch\n",
        "learning_rate = 0.0001       # learning rate\n",
        "\n",
        "# the path where checkpoint saved\n",
        "model_path = './model.ckpt'\n",
        "\n",
        "# create model, define a loss function, and optimizer\n",
        "model = Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdMWsBs7zzNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c00ed511-119d-4ac8-a9a8-094188a64249"
      },
      "source": [
        "# start training\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(num_epoch):\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    # training\n",
        "    model.train() # set the model to training mode\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(inputs) \n",
        "        batch_loss = criterion(outputs, labels)\n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "        batch_loss.backward() \n",
        "        optimizer.step() \n",
        "\n",
        "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "    # validation\n",
        "    if len(val_set) > 0:\n",
        "        model.eval() # set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                batch_loss = criterion(outputs, labels) \n",
        "                _, val_pred = torch.max(outputs, 1) \n",
        "            \n",
        "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "                val_loss += batch_loss.item()\n",
        "\n",
        "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
        "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
        "            ))\n",
        "\n",
        "            # if the model improves, save a checkpoint at this epoch\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
        "    else:\n",
        "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
        "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
        "        ))\n",
        "\n",
        "# if not validating, save the last epoch\n",
        "if len(val_set) == 0:\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print('saving model at last epoch')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[001/500] Train Acc: 0.444833 Loss: 1.916921 | Val Acc: 0.594338 loss: 1.337721\n",
            "saving model with acc 0.594\n",
            "[002/500] Train Acc: 0.563138 Loss: 1.453218 | Val Acc: 0.650454 loss: 1.142277\n",
            "saving model with acc 0.650\n",
            "[003/500] Train Acc: 0.597678 Loss: 1.330238 | Val Acc: 0.673342 loss: 1.054685\n",
            "saving model with acc 0.673\n",
            "[004/500] Train Acc: 0.617540 Loss: 1.258094 | Val Acc: 0.685399 loss: 1.006796\n",
            "saving model with acc 0.685\n",
            "[005/500] Train Acc: 0.631257 Loss: 1.208597 | Val Acc: 0.693888 loss: 0.968698\n",
            "saving model with acc 0.694\n",
            "[006/500] Train Acc: 0.641507 Loss: 1.170276 | Val Acc: 0.702888 loss: 0.938935\n",
            "saving model with acc 0.703\n",
            "[007/500] Train Acc: 0.648693 Loss: 1.141559 | Val Acc: 0.708035 loss: 0.913117\n",
            "saving model with acc 0.708\n",
            "[008/500] Train Acc: 0.656208 Loss: 1.115612 | Val Acc: 0.712986 loss: 0.897461\n",
            "saving model with acc 0.713\n",
            "[009/500] Train Acc: 0.662554 Loss: 1.093083 | Val Acc: 0.715962 loss: 0.882281\n",
            "saving model with acc 0.716\n",
            "[010/500] Train Acc: 0.667657 Loss: 1.074763 | Val Acc: 0.718702 loss: 0.874459\n",
            "saving model with acc 0.719\n",
            "[011/500] Train Acc: 0.671610 Loss: 1.058233 | Val Acc: 0.723027 loss: 0.857784\n",
            "saving model with acc 0.723\n",
            "[012/500] Train Acc: 0.675853 Loss: 1.043657 | Val Acc: 0.725393 loss: 0.846774\n",
            "saving model with acc 0.725\n",
            "[013/500] Train Acc: 0.679983 Loss: 1.029153 | Val Acc: 0.728938 loss: 0.839110\n",
            "saving model with acc 0.729\n",
            "[014/500] Train Acc: 0.682321 Loss: 1.018198 | Val Acc: 0.730897 loss: 0.828770\n",
            "saving model with acc 0.731\n",
            "[015/500] Train Acc: 0.686696 Loss: 1.004571 | Val Acc: 0.732564 loss: 0.824206\n",
            "saving model with acc 0.733\n",
            "[016/500] Train Acc: 0.689791 Loss: 0.992828 | Val Acc: 0.734052 loss: 0.815630\n",
            "saving model with acc 0.734\n",
            "[017/500] Train Acc: 0.691866 Loss: 0.983421 | Val Acc: 0.736410 loss: 0.809303\n",
            "saving model with acc 0.736\n",
            "[018/500] Train Acc: 0.694869 Loss: 0.974530 | Val Acc: 0.737532 loss: 0.803304\n",
            "saving model with acc 0.738\n",
            "[019/500] Train Acc: 0.697145 Loss: 0.965157 | Val Acc: 0.737816 loss: 0.801884\n",
            "saving model with acc 0.738\n",
            "[020/500] Train Acc: 0.699837 Loss: 0.955996 | Val Acc: 0.740264 loss: 0.794326\n",
            "saving model with acc 0.740\n",
            "[021/500] Train Acc: 0.701761 Loss: 0.947397 | Val Acc: 0.742955 loss: 0.785307\n",
            "saving model with acc 0.743\n",
            "[022/500] Train Acc: 0.703658 Loss: 0.941010 | Val Acc: 0.742207 loss: 0.787583\n",
            "[023/500] Train Acc: 0.705995 Loss: 0.933613 | Val Acc: 0.743483 loss: 0.780877\n",
            "saving model with acc 0.743\n",
            "[024/500] Train Acc: 0.707639 Loss: 0.926379 | Val Acc: 0.744532 loss: 0.779669\n",
            "saving model with acc 0.745\n",
            "[025/500] Train Acc: 0.709185 Loss: 0.921292 | Val Acc: 0.745435 loss: 0.776346\n",
            "saving model with acc 0.745\n",
            "[026/500] Train Acc: 0.711079 Loss: 0.914390 | Val Acc: 0.745988 loss: 0.774044\n",
            "saving model with acc 0.746\n",
            "[027/500] Train Acc: 0.712144 Loss: 0.909170 | Val Acc: 0.748264 loss: 0.767183\n",
            "saving model with acc 0.748\n",
            "[028/500] Train Acc: 0.714136 Loss: 0.902189 | Val Acc: 0.748955 loss: 0.766521\n",
            "saving model with acc 0.749\n",
            "[029/500] Train Acc: 0.715513 Loss: 0.897335 | Val Acc: 0.749801 loss: 0.763995\n",
            "saving model with acc 0.750\n",
            "[030/500] Train Acc: 0.717532 Loss: 0.890588 | Val Acc: 0.749427 loss: 0.761748\n",
            "[031/500] Train Acc: 0.718247 Loss: 0.887300 | Val Acc: 0.750638 loss: 0.762075\n",
            "saving model with acc 0.751\n",
            "[032/500] Train Acc: 0.719609 Loss: 0.881720 | Val Acc: 0.750996 loss: 0.758441\n",
            "saving model with acc 0.751\n",
            "[033/500] Train Acc: 0.721412 Loss: 0.874949 | Val Acc: 0.752289 loss: 0.758522\n",
            "saving model with acc 0.752\n",
            "[034/500] Train Acc: 0.722353 Loss: 0.871764 | Val Acc: 0.753850 loss: 0.751473\n",
            "saving model with acc 0.754\n",
            "[035/500] Train Acc: 0.723928 Loss: 0.868031 | Val Acc: 0.752785 loss: 0.754490\n",
            "[036/500] Train Acc: 0.725404 Loss: 0.862631 | Val Acc: 0.754248 loss: 0.750025\n",
            "saving model with acc 0.754\n",
            "[037/500] Train Acc: 0.726088 Loss: 0.857968 | Val Acc: 0.755037 loss: 0.747150\n",
            "saving model with acc 0.755\n",
            "[038/500] Train Acc: 0.727393 Loss: 0.854548 | Val Acc: 0.753947 loss: 0.751168\n",
            "[039/500] Train Acc: 0.728285 Loss: 0.850400 | Val Acc: 0.755874 loss: 0.745587\n",
            "saving model with acc 0.756\n",
            "[040/500] Train Acc: 0.729633 Loss: 0.846475 | Val Acc: 0.755777 loss: 0.747222\n",
            "[041/500] Train Acc: 0.730470 Loss: 0.843276 | Val Acc: 0.755825 loss: 0.743946\n",
            "[042/500] Train Acc: 0.731565 Loss: 0.838863 | Val Acc: 0.755866 loss: 0.742984\n",
            "[043/500] Train Acc: 0.732693 Loss: 0.835655 | Val Acc: 0.756118 loss: 0.741703\n",
            "saving model with acc 0.756\n",
            "[044/500] Train Acc: 0.733321 Loss: 0.832576 | Val Acc: 0.758257 loss: 0.737985\n",
            "saving model with acc 0.758\n",
            "[045/500] Train Acc: 0.734543 Loss: 0.828225 | Val Acc: 0.758330 loss: 0.735204\n",
            "saving model with acc 0.758\n",
            "[046/500] Train Acc: 0.734560 Loss: 0.826755 | Val Acc: 0.757671 loss: 0.738142\n",
            "[047/500] Train Acc: 0.736112 Loss: 0.822761 | Val Acc: 0.758793 loss: 0.734956\n",
            "saving model with acc 0.759\n",
            "[048/500] Train Acc: 0.737123 Loss: 0.818801 | Val Acc: 0.759696 loss: 0.734164\n",
            "saving model with acc 0.760\n",
            "[049/500] Train Acc: 0.737793 Loss: 0.816084 | Val Acc: 0.758045 loss: 0.735831\n",
            "[050/500] Train Acc: 0.738339 Loss: 0.813493 | Val Acc: 0.758387 loss: 0.737046\n",
            "[051/500] Train Acc: 0.739987 Loss: 0.810558 | Val Acc: 0.759923 loss: 0.732265\n",
            "saving model with acc 0.760\n",
            "[052/500] Train Acc: 0.739866 Loss: 0.808299 | Val Acc: 0.760655 loss: 0.730792\n",
            "saving model with acc 0.761\n",
            "[053/500] Train Acc: 0.740893 Loss: 0.804937 | Val Acc: 0.760964 loss: 0.730525\n",
            "saving model with acc 0.761\n",
            "[054/500] Train Acc: 0.741408 Loss: 0.801733 | Val Acc: 0.761216 loss: 0.729462\n",
            "saving model with acc 0.761\n",
            "[055/500] Train Acc: 0.742453 Loss: 0.799680 | Val Acc: 0.760671 loss: 0.732453\n",
            "[056/500] Train Acc: 0.742677 Loss: 0.797749 | Val Acc: 0.761175 loss: 0.728599\n",
            "[057/500] Train Acc: 0.744402 Loss: 0.794115 | Val Acc: 0.761509 loss: 0.728178\n",
            "saving model with acc 0.762\n",
            "[058/500] Train Acc: 0.744835 Loss: 0.791914 | Val Acc: 0.761501 loss: 0.728019\n",
            "[059/500] Train Acc: 0.744981 Loss: 0.790333 | Val Acc: 0.761403 loss: 0.729680\n",
            "[060/500] Train Acc: 0.745726 Loss: 0.787383 | Val Acc: 0.761021 loss: 0.727821\n",
            "[061/500] Train Acc: 0.746237 Loss: 0.785017 | Val Acc: 0.763354 loss: 0.726883\n",
            "saving model with acc 0.763\n",
            "[062/500] Train Acc: 0.746854 Loss: 0.783295 | Val Acc: 0.762322 loss: 0.726886\n",
            "[063/500] Train Acc: 0.748055 Loss: 0.781135 | Val Acc: 0.762761 loss: 0.725338\n",
            "[064/500] Train Acc: 0.747813 Loss: 0.778917 | Val Acc: 0.763484 loss: 0.723848\n",
            "saving model with acc 0.763\n",
            "[065/500] Train Acc: 0.749339 Loss: 0.775433 | Val Acc: 0.762745 loss: 0.725281\n",
            "[066/500] Train Acc: 0.749238 Loss: 0.775165 | Val Acc: 0.762558 loss: 0.724473\n",
            "[067/500] Train Acc: 0.749723 Loss: 0.772598 | Val Acc: 0.762151 loss: 0.728718\n",
            "[068/500] Train Acc: 0.749911 Loss: 0.771815 | Val Acc: 0.763094 loss: 0.724358\n",
            "[069/500] Train Acc: 0.751322 Loss: 0.768757 | Val Acc: 0.762736 loss: 0.723826\n",
            "[070/500] Train Acc: 0.751690 Loss: 0.767005 | Val Acc: 0.763980 loss: 0.720527\n",
            "saving model with acc 0.764\n",
            "[071/500] Train Acc: 0.752074 Loss: 0.765207 | Val Acc: 0.763997 loss: 0.722492\n",
            "saving model with acc 0.764\n",
            "[072/500] Train Acc: 0.752418 Loss: 0.763196 | Val Acc: 0.764216 loss: 0.723707\n",
            "saving model with acc 0.764\n",
            "[073/500] Train Acc: 0.753233 Loss: 0.761870 | Val Acc: 0.765338 loss: 0.721215\n",
            "saving model with acc 0.765\n",
            "[074/500] Train Acc: 0.753203 Loss: 0.759559 | Val Acc: 0.764549 loss: 0.721437\n",
            "[075/500] Train Acc: 0.754153 Loss: 0.757352 | Val Acc: 0.764590 loss: 0.720950\n",
            "[076/500] Train Acc: 0.754164 Loss: 0.757524 | Val Acc: 0.764549 loss: 0.719134\n",
            "[077/500] Train Acc: 0.754612 Loss: 0.755363 | Val Acc: 0.764590 loss: 0.719768\n",
            "[078/500] Train Acc: 0.755539 Loss: 0.753929 | Val Acc: 0.764021 loss: 0.720659\n",
            "[079/500] Train Acc: 0.756046 Loss: 0.752149 | Val Acc: 0.764891 loss: 0.720433\n",
            "[080/500] Train Acc: 0.756349 Loss: 0.749079 | Val Acc: 0.764932 loss: 0.722607\n",
            "[081/500] Train Acc: 0.756376 Loss: 0.748395 | Val Acc: 0.765102 loss: 0.721322\n",
            "[082/500] Train Acc: 0.757286 Loss: 0.746302 | Val Acc: 0.765484 loss: 0.719546\n",
            "saving model with acc 0.765\n",
            "[083/500] Train Acc: 0.757481 Loss: 0.745347 | Val Acc: 0.765452 loss: 0.718648\n",
            "[084/500] Train Acc: 0.757689 Loss: 0.745007 | Val Acc: 0.766062 loss: 0.716647\n",
            "saving model with acc 0.766\n",
            "[085/500] Train Acc: 0.757983 Loss: 0.743295 | Val Acc: 0.764037 loss: 0.718259\n",
            "[086/500] Train Acc: 0.758578 Loss: 0.740684 | Val Acc: 0.765273 loss: 0.717550\n",
            "[087/500] Train Acc: 0.759324 Loss: 0.740085 | Val Acc: 0.764818 loss: 0.719393\n",
            "[088/500] Train Acc: 0.759176 Loss: 0.740152 | Val Acc: 0.765493 loss: 0.719142\n",
            "[089/500] Train Acc: 0.759485 Loss: 0.737972 | Val Acc: 0.765070 loss: 0.719851\n",
            "[090/500] Train Acc: 0.759834 Loss: 0.736791 | Val Acc: 0.765184 loss: 0.716549\n",
            "[091/500] Train Acc: 0.759712 Loss: 0.735752 | Val Acc: 0.765257 loss: 0.719527\n",
            "[092/500] Train Acc: 0.760971 Loss: 0.732339 | Val Acc: 0.766615 loss: 0.715746\n",
            "saving model with acc 0.767\n",
            "[093/500] Train Acc: 0.760956 Loss: 0.732970 | Val Acc: 0.765257 loss: 0.720262\n",
            "[094/500] Train Acc: 0.761437 Loss: 0.732850 | Val Acc: 0.765582 loss: 0.718283\n",
            "[095/500] Train Acc: 0.761448 Loss: 0.730007 | Val Acc: 0.766184 loss: 0.716611\n",
            "[096/500] Train Acc: 0.762476 Loss: 0.728907 | Val Acc: 0.765802 loss: 0.717657\n",
            "[097/500] Train Acc: 0.762609 Loss: 0.727792 | Val Acc: 0.766672 loss: 0.717114\n",
            "saving model with acc 0.767\n",
            "[098/500] Train Acc: 0.762895 Loss: 0.726397 | Val Acc: 0.765525 loss: 0.718750\n",
            "[099/500] Train Acc: 0.763370 Loss: 0.725082 | Val Acc: 0.766086 loss: 0.717283\n",
            "[100/500] Train Acc: 0.763541 Loss: 0.723721 | Val Acc: 0.765777 loss: 0.716877\n",
            "[101/500] Train Acc: 0.763641 Loss: 0.721943 | Val Acc: 0.766525 loss: 0.717221\n",
            "[102/500] Train Acc: 0.763974 Loss: 0.721516 | Val Acc: 0.766550 loss: 0.714640\n",
            "[103/500] Train Acc: 0.764465 Loss: 0.720922 | Val Acc: 0.765688 loss: 0.717447\n",
            "[104/500] Train Acc: 0.764915 Loss: 0.719234 | Val Acc: 0.766785 loss: 0.715966\n",
            "saving model with acc 0.767\n",
            "[105/500] Train Acc: 0.765324 Loss: 0.718366 | Val Acc: 0.766566 loss: 0.715419\n",
            "[106/500] Train Acc: 0.765167 Loss: 0.718151 | Val Acc: 0.766379 loss: 0.715404\n",
            "[107/500] Train Acc: 0.765486 Loss: 0.716434 | Val Acc: 0.767021 loss: 0.715068\n",
            "saving model with acc 0.767\n",
            "[108/500] Train Acc: 0.765884 Loss: 0.715520 | Val Acc: 0.766257 loss: 0.718884\n",
            "[109/500] Train Acc: 0.765923 Loss: 0.713668 | Val Acc: 0.767647 loss: 0.713325\n",
            "saving model with acc 0.768\n",
            "[110/500] Train Acc: 0.766514 Loss: 0.711997 | Val Acc: 0.766680 loss: 0.713131\n",
            "[111/500] Train Acc: 0.766616 Loss: 0.713094 | Val Acc: 0.765932 loss: 0.716233\n",
            "[112/500] Train Acc: 0.767015 Loss: 0.711695 | Val Acc: 0.766696 loss: 0.713982\n",
            "[113/500] Train Acc: 0.767273 Loss: 0.710497 | Val Acc: 0.767078 loss: 0.713575\n",
            "[114/500] Train Acc: 0.767849 Loss: 0.710121 | Val Acc: 0.767785 loss: 0.717038\n",
            "saving model with acc 0.768\n",
            "[115/500] Train Acc: 0.767668 Loss: 0.708776 | Val Acc: 0.767867 loss: 0.713913\n",
            "saving model with acc 0.768\n",
            "[116/500] Train Acc: 0.767970 Loss: 0.707793 | Val Acc: 0.767989 loss: 0.711466\n",
            "saving model with acc 0.768\n",
            "[117/500] Train Acc: 0.768953 Loss: 0.705387 | Val Acc: 0.768322 loss: 0.712420\n",
            "saving model with acc 0.768\n",
            "[118/500] Train Acc: 0.768286 Loss: 0.705843 | Val Acc: 0.767924 loss: 0.712738\n",
            "[119/500] Train Acc: 0.768793 Loss: 0.704822 | Val Acc: 0.767233 loss: 0.717151\n",
            "[120/500] Train Acc: 0.769750 Loss: 0.703163 | Val Acc: 0.767891 loss: 0.713924\n",
            "[121/500] Train Acc: 0.769538 Loss: 0.702304 | Val Acc: 0.767981 loss: 0.714402\n",
            "[122/500] Train Acc: 0.769971 Loss: 0.703051 | Val Acc: 0.767647 loss: 0.716221\n",
            "[123/500] Train Acc: 0.769974 Loss: 0.702750 | Val Acc: 0.767135 loss: 0.715415\n",
            "[124/500] Train Acc: 0.769973 Loss: 0.701669 | Val Acc: 0.768070 loss: 0.714314\n",
            "[125/500] Train Acc: 0.770678 Loss: 0.700207 | Val Acc: 0.768493 loss: 0.713296\n",
            "saving model with acc 0.768\n",
            "[126/500] Train Acc: 0.770472 Loss: 0.699301 | Val Acc: 0.768159 loss: 0.715693\n",
            "[127/500] Train Acc: 0.770838 Loss: 0.699373 | Val Acc: 0.769062 loss: 0.712568\n",
            "saving model with acc 0.769\n",
            "[128/500] Train Acc: 0.770390 Loss: 0.698529 | Val Acc: 0.768745 loss: 0.714271\n",
            "[129/500] Train Acc: 0.771008 Loss: 0.696937 | Val Acc: 0.767850 loss: 0.715253\n",
            "[130/500] Train Acc: 0.771387 Loss: 0.696463 | Val Acc: 0.767826 loss: 0.713547\n",
            "[131/500] Train Acc: 0.771934 Loss: 0.695017 | Val Acc: 0.769005 loss: 0.709996\n",
            "[132/500] Train Acc: 0.771849 Loss: 0.694900 | Val Acc: 0.768127 loss: 0.711209\n",
            "[133/500] Train Acc: 0.772573 Loss: 0.693191 | Val Acc: 0.768298 loss: 0.713658\n",
            "[134/500] Train Acc: 0.772196 Loss: 0.693416 | Val Acc: 0.769477 loss: 0.710785\n",
            "saving model with acc 0.769\n",
            "[135/500] Train Acc: 0.772629 Loss: 0.691035 | Val Acc: 0.767688 loss: 0.714355\n",
            "[136/500] Train Acc: 0.772712 Loss: 0.691683 | Val Acc: 0.767476 loss: 0.714970\n",
            "[137/500] Train Acc: 0.773185 Loss: 0.691324 | Val Acc: 0.769046 loss: 0.710437\n",
            "[138/500] Train Acc: 0.773374 Loss: 0.689499 | Val Acc: 0.769493 loss: 0.710819\n",
            "saving model with acc 0.769\n",
            "[139/500] Train Acc: 0.773374 Loss: 0.688990 | Val Acc: 0.769078 loss: 0.712124\n",
            "[140/500] Train Acc: 0.773501 Loss: 0.688940 | Val Acc: 0.768842 loss: 0.713881\n",
            "[141/500] Train Acc: 0.773855 Loss: 0.687873 | Val Acc: 0.768785 loss: 0.710457\n",
            "[142/500] Train Acc: 0.773410 Loss: 0.687836 | Val Acc: 0.770119 loss: 0.709006\n",
            "saving model with acc 0.770\n",
            "[143/500] Train Acc: 0.774436 Loss: 0.686412 | Val Acc: 0.769086 loss: 0.707420\n",
            "[144/500] Train Acc: 0.774483 Loss: 0.685799 | Val Acc: 0.769290 loss: 0.709125\n",
            "[145/500] Train Acc: 0.773911 Loss: 0.684856 | Val Acc: 0.768493 loss: 0.711786\n",
            "[146/500] Train Acc: 0.775026 Loss: 0.684933 | Val Acc: 0.768672 loss: 0.710372\n",
            "[147/500] Train Acc: 0.775369 Loss: 0.682984 | Val Acc: 0.768899 loss: 0.711451\n",
            "[148/500] Train Acc: 0.774920 Loss: 0.683085 | Val Acc: 0.768314 loss: 0.712517\n",
            "[149/500] Train Acc: 0.775322 Loss: 0.682844 | Val Acc: 0.768794 loss: 0.710576\n",
            "[150/500] Train Acc: 0.775495 Loss: 0.682273 | Val Acc: 0.769200 loss: 0.707603\n",
            "[151/500] Train Acc: 0.775320 Loss: 0.681610 | Val Acc: 0.769029 loss: 0.710826\n",
            "[152/500] Train Acc: 0.776335 Loss: 0.680123 | Val Acc: 0.769802 loss: 0.709843\n",
            "[153/500] Train Acc: 0.776574 Loss: 0.680452 | Val Acc: 0.769322 loss: 0.713112\n",
            "[154/500] Train Acc: 0.775978 Loss: 0.679171 | Val Acc: 0.768582 loss: 0.712022\n",
            "[155/500] Train Acc: 0.776446 Loss: 0.679259 | Val Acc: 0.770168 loss: 0.709465\n",
            "saving model with acc 0.770\n",
            "[156/500] Train Acc: 0.776950 Loss: 0.677484 | Val Acc: 0.769834 loss: 0.710170\n",
            "[157/500] Train Acc: 0.776561 Loss: 0.676920 | Val Acc: 0.770119 loss: 0.708476\n",
            "[158/500] Train Acc: 0.776602 Loss: 0.676993 | Val Acc: 0.769964 loss: 0.710371\n",
            "[159/500] Train Acc: 0.777387 Loss: 0.676210 | Val Acc: 0.769371 loss: 0.710621\n",
            "[160/500] Train Acc: 0.776884 Loss: 0.675894 | Val Acc: 0.769924 loss: 0.710584\n",
            "[161/500] Train Acc: 0.777601 Loss: 0.674491 | Val Acc: 0.769761 loss: 0.709623\n",
            "[162/500] Train Acc: 0.777427 Loss: 0.674323 | Val Acc: 0.770314 loss: 0.710774\n",
            "saving model with acc 0.770\n",
            "[163/500] Train Acc: 0.778569 Loss: 0.672624 | Val Acc: 0.768590 loss: 0.712428\n",
            "[164/500] Train Acc: 0.778362 Loss: 0.673158 | Val Acc: 0.770257 loss: 0.711617\n",
            "[165/500] Train Acc: 0.777976 Loss: 0.672733 | Val Acc: 0.769623 loss: 0.708370\n",
            "[166/500] Train Acc: 0.778046 Loss: 0.673697 | Val Acc: 0.768582 loss: 0.708457\n",
            "[167/500] Train Acc: 0.778749 Loss: 0.670879 | Val Acc: 0.769038 loss: 0.710729\n",
            "[168/500] Train Acc: 0.778465 Loss: 0.670471 | Val Acc: 0.769712 loss: 0.710612\n",
            "[169/500] Train Acc: 0.778766 Loss: 0.670004 | Val Acc: 0.768883 loss: 0.714569\n",
            "[170/500] Train Acc: 0.778909 Loss: 0.670220 | Val Acc: 0.769346 loss: 0.707635\n",
            "[171/500] Train Acc: 0.778823 Loss: 0.670398 | Val Acc: 0.770501 loss: 0.709041\n",
            "saving model with acc 0.771\n",
            "[172/500] Train Acc: 0.779928 Loss: 0.668241 | Val Acc: 0.770477 loss: 0.706683\n",
            "[173/500] Train Acc: 0.779401 Loss: 0.667992 | Val Acc: 0.770168 loss: 0.710923\n",
            "[174/500] Train Acc: 0.779629 Loss: 0.667308 | Val Acc: 0.771168 loss: 0.706229\n",
            "saving model with acc 0.771\n",
            "[175/500] Train Acc: 0.779502 Loss: 0.669202 | Val Acc: 0.768298 loss: 0.712066\n",
            "[176/500] Train Acc: 0.779790 Loss: 0.666741 | Val Acc: 0.770525 loss: 0.706454\n",
            "[177/500] Train Acc: 0.780208 Loss: 0.665922 | Val Acc: 0.769696 loss: 0.708499\n",
            "[178/500] Train Acc: 0.780119 Loss: 0.665932 | Val Acc: 0.771005 loss: 0.707782\n",
            "[179/500] Train Acc: 0.780803 Loss: 0.665682 | Val Acc: 0.769729 loss: 0.710355\n",
            "[180/500] Train Acc: 0.780546 Loss: 0.664925 | Val Acc: 0.770216 loss: 0.709445\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hi7jTn3PX-m"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfUECMFCn5VG"
      },
      "source": [
        "Create a testing dataset, and load model from the saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PKjtAScPWtr"
      },
      "source": [
        "# create testing dataset\n",
        "test_set = TIMITDataset(test, None)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# create model and load weights from checkpoint\n",
        "model = Classifier().to(device)\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "940TtCCdoYd0"
      },
      "source": [
        "Make prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84HU5GGjPqR0"
      },
      "source": [
        "predict = []\n",
        "model.eval() # set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs = data\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "\n",
        "        for y in test_pred.cpu().numpy():\n",
        "            predict.append(y)\n",
        "\n",
        "        for i, data in enumerate(test_loader):\n",
        "          if i != 0 & i != len(predict) - 1:\n",
        "            if predict[i] != predict[i-1] & predict[i] != predict[i+1]:\n",
        "              predict[i] = predict[i+1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWDf_C-omElb"
      },
      "source": [
        "Write prediction to a CSV file.\n",
        "\n",
        "After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuljYSPHcZir"
      },
      "source": [
        "with open('prediction.csv', 'w') as f:\n",
        "    f.write('Id,Class\\n')\n",
        "    for i, y in enumerate(predict):\n",
        "        f.write('{},{}\\n'.format(i, y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "133iBJGTs97s"
      },
      "source": [
        "# !kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}